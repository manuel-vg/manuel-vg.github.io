---
title: "Prepositions Matter in Quantifier Scope Disambiguation"
collection: publications
category: conferences
permalink: /publication/2022-10-12-prepositions-matter-in-quantifier-scope-disambiguation
excerpt: 'This paper shows that incorporating prepositional sense information significantly improves machine learning models for quantifier scope disambiguation (QSD).'
date: 2022-10-12
venue: '29th International Conference on Computational Linguistics'
paperurl: 'https://aclanthology.org/2022.coling-1.348.pdf'
citation: 'Aleksander Leczkowski, Justyna Grudzińska, Manuel Vargas Guzmán, Aleksander Wawer, and Aleksandra Siemieniuk. (2022). &quot;Prepositions Matter in Quantifier Scope Disambiguation.&quot; <i>29th International Conference on Computational Linguistics</i>. Pages 3960–3970.'
---

Although it is widely agreed that world knowledge plays a significant role in quantifier scope disambiguation (QSD), there has been only very limited work on how to integrate this knowledge into a QSD model. This paper contributes to this scarce line of research by incorporating into a machine learning model our knowledge about relations, as conveyed by a manageable closed class of function words: prepositions. For data, we use a scope-disambiguated corpus created by AnderBois, Brasoveanu and Henderson, which is additionally annotated with prepositional senses using Schneider et al’s Semantic Network of Adposition and Case Supersenses (SNACS) scheme. By applying Manshadi and Allen’s method to the corpus, we were able to inspect the information gain provided by prepositions for the QSD task. Statistical analysis of the performance of the classifiers, trained in scenarios with and without preposition information, supports the claim that prepositional senses have a strong positive impact on the learnability of automatic QSD systems.
